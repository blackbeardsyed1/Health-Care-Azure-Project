# Healthcare Revenue Cycle Management (RCM) Data Pipeline

## 📌 Project Overview

This project implements an end-to-end **Healthcare Revenue Cycle Management (RCM)** data pipeline leveraging the **Azure Data Engineering stack**. It streamlines the ingestion, processing, and transformation of diverse healthcare datasets into actionable insights using the **Medallion Architecture** (Landing → Bronze → Silver → Gold), empowering analysts and stakeholders with comprehensive analytics.

---

## 🔍 Problem Domain

Revenue Cycle Management (RCM) orchestrates the financial lifecycle of healthcare operations, encompassing:

* **Patient Visits**: Capturing patient and insurance details.
* **Service Delivery**: Documenting services rendered.
* **Billing and Claims**: Generating bills and managing insurance claims.
* **Payments and Follow-Ups**: Processing payments, collections, and adjustments.
* **Performance Tracking**: Monitoring key financial metrics.

### Critical Components:

* **Accounts Receivable (AR)**: Optimizing cash flow by minimizing collection periods.
* **Accounts Payable (AP)**: Ensuring timely vendor and supplier payments.

---

## 🛠️ Solution Architecture

Employing the **Medallion Architecture**, the pipeline handles datasets such as:

### 📂 Datasets

* **EMR Data** (Azure SQL DB): Patients, Providers, Departments, Transactions, Encounters
* **Claims Data** (Flat files, monthly): Insurance claims ingested from Azure Data Lake
* **Public APIs**: NPI Data, ICD Codes

---

## 🌊 Data Pipeline Layers

* **Landing**: Raw data ingestion (flat files, SQL DB, APIs)
* **Bronze**: Raw data stored in Parquet (Single source of truth)
* **Silver**: Cleaned and enriched data modeled into **Common Data Model (CDM)** with historical tracking (**SCD Type 2**)
* **Gold**: Aggregated Fact & Dimension tables for analytics

---

## 🚀 Technologies Used

* **Azure Data Factory**: Data ingestion and orchestration
* **Azure Databricks**: Data processing and transformation
* **Azure Data Lake Storage Gen2**: Scalable hierarchical data storage
* **Delta Lake**: ACID transactions, data versioning, efficient management
* **Azure Key Vault**: Secure credential management

---

## ⚙️ Key Features

### 🔄 Automated Ingestion Pipeline

* Metadata-driven ingestion via `load_config.csv`
* Comprehensive audit logging

### 🥉 Bronze Layer

* Raw data persistence in Parquet
* Supports incremental loads with watermarks

### 🥈 Silver Layer

* Data cleansing: duplicates removal, validation
* Historical data management using SCD Type 2
* Data standardization (ICD, NPI codes)

### 🥇 Gold Layer

* Aggregated **Fact tables**: AR Metrics (AR > 90 Days, Days in AR)
* Contextual **Dimension tables**: Patients, Providers, Departments

### 🔀 Parallel Processing

* Optimized for scalability and performance

### 🔒 Unified Catalog

* Secure, organized management via Unity Catalog

---

## 📈 Metrics and KPIs

* **AR > 90 Days**: Percentage of receivables overdue beyond 90 days.

  ```
  Example: AR > 90 Days = $200K / $1M = 20%
  ```
* **Days in AR**: Average duration for receivable collection.

  ```
  Example: Days in AR = $400K / $10K per day = 40 Days
  ```

---

## 🚧 Data Pipeline Implementation

### Bronze Layer

* Reads configurations (`load_config.csv`) using Azure Data Factory
* Processes data incrementally or fully based on metadata
* Archives processed files
* Detailed audit logging

### Silver Layer

* Robust data cleaning and enrichment
* Implements SCD Type 2 for dimension history

### Gold Layer

* Creates actionable analytics tables (Fact & Dimension)
* Enables detailed financial insights

---

## 🖥️ Deployment and Configuration

### Azure Services Setup

* **Azure Data Factory**: Configured Linked Services and pipeline activities
* **Azure Databricks**: Transformation scripts, Delta Lake management
* **ADLS Gen2 Storage Structure**:

  ```
  ttadlsdev/
  ├── landing/
  ├── bronze/
  ├── silver/
  ├── gold/
  ```

### Pipeline Execution

* Update `load_config.csv` with relevant paths
* Trigger pipelines in Azure Data Factory

### Data Access

* Gold layer integration with BI tools (**Power BI**, **Tableau**)

---

## 📊 Monitoring and Analytics

* **Azure Monitor and Log Analytics** for pipeline performance
* Integrated dashboards for tracking pipeline health and data metrics

---

## ✅ How to Run

1. Deploy Azure resources (SQL DB, ADLS, Databricks, Key Vault)
2. Update pipeline configurations (`load_config.csv`)
3. Execute Azure Data Factory pipelines
4. Access insights in BI tools

---

🚩 **Empower your healthcare operations with streamlined, data-driven revenue cycle management analytics.**
