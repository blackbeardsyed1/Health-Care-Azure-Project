# Healthcare Revenue Cycle Management (RCM) Data Pipeline

## ğŸ“Œ Project Overview

This project implements an end-to-end **Healthcare Revenue Cycle Management (RCM)** data pipeline leveraging the **Azure Data Engineering stack**. It streamlines the ingestion, processing, and transformation of diverse healthcare datasets into actionable insights using the **Medallion Architecture** (Landing â†’ Bronze â†’ Silver â†’ Gold), empowering analysts and stakeholders with comprehensive analytics.

---

## ğŸ” Problem Domain

Revenue Cycle Management (RCM) orchestrates the financial lifecycle of healthcare operations, encompassing:

* **Patient Visits**: Capturing patient and insurance details.
* **Service Delivery**: Documenting services rendered.
* **Billing and Claims**: Generating bills and managing insurance claims.
* **Payments and Follow-Ups**: Processing payments, collections, and adjustments.
* **Performance Tracking**: Monitoring key financial metrics.

### Critical Components:

* **Accounts Receivable (AR)**: Optimizing cash flow by minimizing collection periods.
* **Accounts Payable (AP)**: Ensuring timely vendor and supplier payments.

---

## ğŸ› ï¸ Solution Architecture

Employing the **Medallion Architecture**, the pipeline handles datasets such as:

### ğŸ“‚ Datasets

* **EMR Data** (Azure SQL DB): Patients, Providers, Departments, Transactions, Encounters
* **Claims Data** (Flat files, monthly): Insurance claims ingested from Azure Data Lake
* **Public APIs**: NPI Data, ICD Codes

---

## ğŸŒŠ Data Pipeline Layers

* **Landing**: Raw data ingestion (flat files, SQL DB, APIs)
* **Bronze**: Raw data stored in Parquet (Single source of truth)
* **Silver**: Cleaned and enriched data modeled into **Common Data Model (CDM)** with historical tracking (**SCD Type 2**)
* **Gold**: Aggregated Fact & Dimension tables for analytics

---

## ğŸš€ Technologies Used

* **Azure Data Factory**: Data ingestion and orchestration
* **Azure Databricks**: Data processing and transformation
* **Azure Data Lake Storage Gen2**: Scalable hierarchical data storage
* **Delta Lake**: ACID transactions, data versioning, efficient management
* **Azure Key Vault**: Secure credential management

---

## âš™ï¸ Key Features

### ğŸ”„ Automated Ingestion Pipeline

* Metadata-driven ingestion via `load_config.csv`
* Comprehensive audit logging

### ğŸ¥‰ Bronze Layer

* Raw data persistence in Parquet
* Supports incremental loads with watermarks

### ğŸ¥ˆ Silver Layer

* Data cleansing: duplicates removal, validation
* Historical data management using SCD Type 2
* Data standardization (ICD, NPI codes)

### ğŸ¥‡ Gold Layer

* Aggregated **Fact tables**: AR Metrics (AR > 90 Days, Days in AR)
* Contextual **Dimension tables**: Patients, Providers, Departments

### ğŸ”€ Parallel Processing

* Optimized for scalability and performance

### ğŸ”’ Unified Catalog

* Secure, organized management via Unity Catalog

---

## ğŸ“ˆ Metrics and KPIs

* **AR > 90 Days**: Percentage of receivables overdue beyond 90 days.

  ```
  Example: AR > 90 Days = $200K / $1M = 20%
  ```
* **Days in AR**: Average duration for receivable collection.

  ```
  Example: Days in AR = $400K / $10K per day = 40 Days
  ```

---

## ğŸš§ Data Pipeline Implementation

### Bronze Layer

* Reads configurations (`load_config.csv`) using Azure Data Factory
* Processes data incrementally or fully based on metadata
* Archives processed files
* Detailed audit logging

### Silver Layer

* Robust data cleaning and enrichment
* Implements SCD Type 2 for dimension history

### Gold Layer

* Creates actionable analytics tables (Fact & Dimension)
* Enables detailed financial insights

---

## ğŸ–¥ï¸ Deployment and Configuration

### Azure Services Setup

* **Azure Data Factory**: Configured Linked Services and pipeline activities
* **Azure Databricks**: Transformation scripts, Delta Lake management
* **ADLS Gen2 Storage Structure**:

  ```
  ttadlsdev/
  â”œâ”€â”€ landing/
  â”œâ”€â”€ bronze/
  â”œâ”€â”€ silver/
  â”œâ”€â”€ gold/
  ```

### Pipeline Execution

* Update `load_config.csv` with relevant paths
* Trigger pipelines in Azure Data Factory

### Data Access

* Gold layer integration with BI tools (**Power BI**, **Tableau**)

---

## ğŸ“Š Monitoring and Analytics

* **Azure Monitor and Log Analytics** for pipeline performance
* Integrated dashboards for tracking pipeline health and data metrics

---

## âœ… How to Run

1. Deploy Azure resources (SQL DB, ADLS, Databricks, Key Vault)
2. Update pipeline configurations (`load_config.csv`)
3. Execute Azure Data Factory pipelines
4. Access insights in BI tools

---

ğŸš© **Empower your healthcare operations with streamlined, data-driven revenue cycle management analytics.**
